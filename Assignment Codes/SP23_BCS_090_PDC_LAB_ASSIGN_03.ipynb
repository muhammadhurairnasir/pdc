{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9dpKDgDqABa",
        "outputId": "d46b1118-be4d-4143-dbd7-22e858aae409"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Array size N = 1024\n",
            "\n",
            "--- 1. Memory Allocation and Initialization ---\n",
            "Host A[:5]: [0 1 2 3 4]\n",
            "Host B[:5]: [0 2 4 6 8]\n",
            "Arrays copied to GPU.\n",
            "\n",
            "--- 2. Serial Kernel Execution ---\n",
            "Kernel1 (A+B) done.\n",
            "Kernel2 (C*C) done.\n",
            "D_serial_host[:5]: [  0   9  36  81 144]\n",
            "\n",
            "--- 3. Parallel Kernel Execution (Streams) ---\n",
            "Kernel1 on stream1 done.\n",
            "Kernel2 on stream2 (after stream1).\n",
            "D_stream_host[:5]: [  0   9  36  81 144]\n",
            "\n",
            "--- 4. Synchronization Demo ---\n",
            "Synchronized with GPU.\n",
            "\n",
            "--- 5. Thread Hierarchy ---\n",
            "\n",
            "<<<1, N>>>: one block with N threads\n",
            "\n",
            "<<<N/32, 32>>>: N/32 blocks, 32 threads each\n",
            "\n",
            "--- 6. Reduction Kernel ---\n",
            "GPU sum = -1078458880, Expected sum = 3216508416\n",
            "Match? False\n",
            "\n",
            "--- Done ---\n"
          ]
        }
      ],
      "source": [
        "import cupy as cp\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# --- Setup CUDA Kernels ---\n",
        "# Using RawKernel only for indexing and reduction demo\n",
        "N = 1024\n",
        "THREADS_PER_BLOCK = 32\n",
        "BLOCKS_N_32 = (N // THREADS_PER_BLOCK, 1, 1) # number of blocks\n",
        "THREADS_32 = (THREADS_PER_BLOCK, 1, 1)       # number of threads\n",
        "\n",
        "CUDA_KERNELS = r'''\n",
        "// Kernel to show thread indexing\n",
        "extern \"C\" __global__ void kernel_indexing(int N) {\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (i == 0 || i == N - 1 || i == N/2) {\n",
        "        printf(\"Thread %d (Total N=%d): BlockIdx=%d, ThreadIdx=%d\\n\", i, N, blockIdx.x, threadIdx.x);\n",
        "    }\n",
        "    if (blockIdx.x == 0 && threadIdx.x == 0) {\n",
        "        printf(\"--- Launch Setup: GridDim=%d, BlockDim=%d ---\\n\", gridDim.x, blockDim.x);\n",
        "    }\n",
        "}\n",
        "\n",
        "// Kernel for sum reduction using shared memory\n",
        "extern \"C\" __global__ void kernel_reduction(const int* D, int* R, int N) {\n",
        "    extern __shared__ int sdata[];\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int tid = threadIdx.x;\n",
        "    sdata[tid] = (i < N) ? D[i] : 0;\n",
        "    __syncthreads();\n",
        "\n",
        "    // reduction loop\n",
        "    for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
        "        if (tid < s) sdata[tid] += sdata[tid + s];\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid == 0) atomicAdd(R, sdata[0]);\n",
        "}\n",
        "'''\n",
        "\n",
        "# Compile kernels\n",
        "k_index = cp.RawKernel(CUDA_KERNELS, 'kernel_indexing')\n",
        "k_reduction = cp.RawKernel(CUDA_KERNELS, 'kernel_reduction')\n",
        "\n",
        "print(f\"Array size N = {N}\\n\")\n",
        "\n",
        "# --- 1. Memory Allocation ---\n",
        "print(\"--- 1. Memory Allocation and Initialization ---\")\n",
        "A_host = np.arange(N, dtype=np.int32)       # A = [0,1,2,...]\n",
        "B_host = 2 * np.arange(N, dtype=np.int32)   # B = [0,2,4,...]\n",
        "print(f\"Host A[:5]: {A_host[:5]}\")\n",
        "print(f\"Host B[:5]: {B_host[:5]}\")\n",
        "\n",
        "A_device = cp.asarray(A_host)  # Copy to GPU\n",
        "B_device = cp.asarray(B_host)\n",
        "print(\"Arrays copied to GPU.\\n\")\n",
        "\n",
        "# --- 2. Simple GPU Ops ---\n",
        "print(\"--- 2. Serial Kernel Execution ---\")\n",
        "C_device = A_device + B_device              # Kernel 1: add\n",
        "print(\"Kernel1 (A+B) done.\")\n",
        "D_device = C_device * C_device              # Kernel 2: square\n",
        "print(\"Kernel2 (C*C) done.\")\n",
        "\n",
        "D_serial_host = D_device.get()              # Copy result back\n",
        "print(f\"D_serial_host[:5]: {D_serial_host[:5]}\")\n",
        "\n",
        "# --- 3. Using Streams ---\n",
        "print(\"\\n--- 3. Parallel Kernel Execution (Streams) ---\")\n",
        "stream1 = cp.cuda.Stream()\n",
        "stream2 = cp.cuda.Stream()\n",
        "event_1 = cp.cuda.Event()\n",
        "\n",
        "D_device.fill(0)\n",
        "with stream1:                               # Run kernel1 on stream1\n",
        "    C_device_stream = A_device + B_device\n",
        "    stream1.record(event_1)\n",
        "    print(\"Kernel1 on stream1 done.\")\n",
        "\n",
        "stream2.wait_event(event_1)                 # Wait for stream1\n",
        "with stream2:                               # Run kernel2 on stream2\n",
        "    D_device_stream = C_device_stream * C_device_stream\n",
        "    print(\"Kernel2 on stream2 (after stream1).\")\n",
        "\n",
        "D_stream_host = D_device_stream.get(stream=stream2)  # async copy\n",
        "stream2.synchronize()\n",
        "print(f\"D_stream_host[:5]: {D_stream_host[:5]}\")\n",
        "\n",
        "# --- 4. Sync Demo ---\n",
        "print(\"\\n--- 4. Synchronization Demo ---\")\n",
        "D_device = (A_device + B_device) * (A_device + B_device)\n",
        "cp.cuda.Device(0).synchronize()             # wait for GPU\n",
        "print(\"Synchronized with GPU.\")\n",
        "\n",
        "# --- 5. Thread Hierarchy Demo ---\n",
        "print(\"\\n--- 5. Thread Hierarchy ---\")\n",
        "print(\"\\n<<<1, N>>>: one block with N threads\")\n",
        "k_index((1, 1, 1), (N, 1, 1), (N,))\n",
        "cp.cuda.Device(0).synchronize()\n",
        "\n",
        "print(\"\\n<<<N/32, 32>>>: N/32 blocks, 32 threads each\")\n",
        "k_index(BLOCKS_N_32, THREADS_32, (N,))\n",
        "cp.cuda.Device(0).synchronize()\n",
        "\n",
        "# --- 6. Reduction ---\n",
        "print(\"\\n--- 6. Reduction Kernel ---\")\n",
        "R_device = cp.zeros(1, dtype=np.int32)\n",
        "s_size = THREADS_PER_BLOCK * np.dtype(np.int32).itemsize\n",
        "k_reduction(BLOCKS_N_32, THREADS_32, (D_device, R_device, N), shared_mem=s_size)\n",
        "cp.cuda.Device(0).synchronize()\n",
        "final_sum_gpu = R_device.get()[0]\n",
        "\n",
        "expected_sum = D_device.sum().get()\n",
        "print(f\"GPU sum = {final_sum_gpu}, Expected sum = {expected_sum}\")\n",
        "print(f\"Match? {final_sum_gpu == expected_sum}\")\n",
        "\n",
        "print(\"\\n--- Done ---\")\n"
      ]
    }
  ]
}